{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11699725,"sourceType":"datasetVersion","datasetId":7343599},{"sourceId":185693,"sourceType":"modelInstanceVersion","modelInstanceId":158308,"modelId":164716}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.model_download(\"google/paligemma-2/transformers/paligemma2-3b-pt-224\")\n\nprint(\"Path to model files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T18:32:40.031918Z","iopub.execute_input":"2025-05-17T18:32:40.032226Z","iopub.status.idle":"2025-05-17T18:32:41.001715Z","shell.execute_reply.started":"2025-05-17T18:32:40.032199Z","shell.execute_reply":"2025-05-17T18:32:41.000801Z"}},"outputs":[{"name":"stdout","text":"Path to model files: /kaggle/input/paligemma-2/transformers/paligemma2-3b-pt-224/1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Install dependencies\n!pip install -q transformers peft nltk rouge-score wandb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verify GPU\nimport torch\nprint(torch.__version__)\nprint(torch.cuda.is_available())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"d070aabfe54f4733fb727662604b037dee34842c\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T18:32:45.029737Z","iopub.execute_input":"2025-05-17T18:32:45.030070Z","iopub.status.idle":"2025-05-17T18:32:51.778708Z","shell.execute_reply.started":"2025-05-17T18:32:45.030044Z","shell.execute_reply":"2025-05-17T18:32:51.778162Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madigew\u001b[0m (\u001b[33madigew-middle-east-technical-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n\ndata = pd.read_csv(\"/kaggle/input/riscmm/RISCM/captions.csv\")\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T18:32:54.630248Z","iopub.execute_input":"2025-05-17T18:32:54.630968Z","iopub.status.idle":"2025-05-17T18:32:55.114885Z","shell.execute_reply.started":"2025-05-17T18:32:54.630943Z","shell.execute_reply":"2025-05-17T18:32:55.114233Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"  source split           image  \\\n0   NWPU  test  NWPU_31430.jpg   \n1   NWPU  test  NWPU_31431.jpg   \n2   NWPU  test  NWPU_31432.jpg   \n3   NWPU  test  NWPU_31433.jpg   \n4   NWPU  test  NWPU_31434.jpg   \n\n                                           caption_1  \\\n0   A gray plane on the runway and the lawn beside .   \n1  Three small planes parked in a line on the air...   \n2  A plane parked in a line on the airport with s...   \n3  A small plane and a big plane parked next to b...   \n4       Two planes parked next to boarding bridges .   \n\n                                           caption_2  \\\n0        A grey plane is on the runway by the lawn .   \n1  There are four aircraft on the open ground, Th...   \n2  A white plane was parked on the instruction li...   \n3  A white plane and a gray plane parked at the b...   \n4  Two aircraft were parked at the departure gates .   \n\n                                           caption_3  \\\n0  There is an airplane on the runway with a larg...   \n1  There are many planes of different sizes in a ...   \n2  An airplane parked in an open area with many c...   \n3  Two planes of different sizes are neatly parke...   \n4  Two planes of different sizes are neatly parke...   \n\n                                           caption_4  \\\n0  A plane is parked on the runway next to the gr...   \n1             Four planes are parked on the runway .   \n2              A plane is parked on the open space .   \n3  A large plane and a small plane are parked nea...   \n4       Two planes are parked next to the terminal .   \n\n                                           caption_5  \n0  There is a plane on the runway beside the grass .  \n1  Four planes of different sizes were on the mar...  \n2            There is 1 plane on the ground marked .  \n3              Two planes are on the marked ground .  \n4              Two planes are on the marked ground .  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>split</th>\n      <th>image</th>\n      <th>caption_1</th>\n      <th>caption_2</th>\n      <th>caption_3</th>\n      <th>caption_4</th>\n      <th>caption_5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NWPU</td>\n      <td>test</td>\n      <td>NWPU_31430.jpg</td>\n      <td>A gray plane on the runway and the lawn beside .</td>\n      <td>A grey plane is on the runway by the lawn .</td>\n      <td>There is an airplane on the runway with a larg...</td>\n      <td>A plane is parked on the runway next to the gr...</td>\n      <td>There is a plane on the runway beside the grass .</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NWPU</td>\n      <td>test</td>\n      <td>NWPU_31431.jpg</td>\n      <td>Three small planes parked in a line on the air...</td>\n      <td>There are four aircraft on the open ground, Th...</td>\n      <td>There are many planes of different sizes in a ...</td>\n      <td>Four planes are parked on the runway .</td>\n      <td>Four planes of different sizes were on the mar...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NWPU</td>\n      <td>test</td>\n      <td>NWPU_31432.jpg</td>\n      <td>A plane parked in a line on the airport with s...</td>\n      <td>A white plane was parked on the instruction li...</td>\n      <td>An airplane parked in an open area with many c...</td>\n      <td>A plane is parked on the open space .</td>\n      <td>There is 1 plane on the ground marked .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NWPU</td>\n      <td>test</td>\n      <td>NWPU_31433.jpg</td>\n      <td>A small plane and a big plane parked next to b...</td>\n      <td>A white plane and a gray plane parked at the b...</td>\n      <td>Two planes of different sizes are neatly parke...</td>\n      <td>A large plane and a small plane are parked nea...</td>\n      <td>Two planes are on the marked ground .</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NWPU</td>\n      <td>test</td>\n      <td>NWPU_31434.jpg</td>\n      <td>Two planes parked next to boarding bridges .</td>\n      <td>Two aircraft were parked at the departure gates .</td>\n      <td>Two planes of different sizes are neatly parke...</td>\n      <td>Two planes are parked next to the terminal .</td>\n      <td>Two planes are on the marked ground .</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"Load Dataset with Small Partition","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\n\ndef load_small_partition(image_dir, caption_file, sample_size=300):\n    df = pd.read_csv(caption_file)\n\n    # Filter out missing images\n    valid_images = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n    df = df[df['image'].isin(valid_images)]\n\n    # Use existing splits\n    train_df = df[df['split'] == 'train'].sample(frac=1, random_state=42).head(sample_size)\n    val_df = df[df['split'] == 'test'].sample(frac=1, random_state=42).head(int(0.2 * sample_size))\n\n    print(f\"Loaded small partition: {len(train_df)} train, {len(val_df)} val\")\n    return train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n\n# Set paths\nimage_dir = \"/kaggle/input/riscmm/RISCM/resized\"\ncaption_file = \"/kaggle/input/riscmm/RISCM/captions.csv\"\n\n# Load data\ntrain_df, val_df = load_small_partition(image_dir, caption_file, sample_size=100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T18:32:57.164959Z","iopub.execute_input":"2025-05-17T18:32:57.165597Z","iopub.status.idle":"2025-05-17T18:34:18.198532Z","shell.execute_reply.started":"2025-05-17T18:32:57.165571Z","shell.execute_reply":"2025-05-17T18:34:18.197770Z"}},"outputs":[{"name":"stdout","text":"Loaded small partition: 100 train, 20 val\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Training Function","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import PaliGemmaForConditionalGeneration, PaliGemmaProcessor\nfrom peft import LoraConfig, get_peft_model\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.amp import autocast, GradScaler\nimport torch.optim as optim\n\nclass RISCDataset(Dataset):\n    def __init__(self, image_dir, df):\n        self.image_dir = image_dir\n        self.df = df\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = os.path.join(self.image_dir, row.image)\n        image = Image.open(image_path).convert('RGB')\n        caption = row.caption_1\n        return {\"image\": image, \"caption\": caption}\n\ndef custom_collate_fn(batch):\n    images = [item[\"image\"] for item in batch]\n    captions = [item[\"caption\"] for item in batch]\n    return {\"images\": images, \"captions\": captions}\n\ndef train_lora(model_name, image_dir, train_df, val_df, caption_file, output_dir,\n               lora_rank=32, epochs=2, learning_rate=5e-4,\n               max_train_samples=None, max_val_samples=None,\n               batch_size=1, accum_steps=8,\n               target_modules=[\"q_proj\", \"v_proj\"]):\n\n    wandb.init(project=\"DI725_Phase2\", name=f\"LoRA-R{lora_rank}\")\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = PaliGemmaForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n    processor = PaliGemmaProcessor.from_pretrained(model_name, use_fast=True)\n\n    lora_config = LoraConfig(\n        r=lora_rank,\n        lora_alpha=32,\n        target_modules=target_modules,\n        lora_dropout=0.1\n    )\n    model = get_peft_model(model, lora_config)\n\n    # Use provided train/val DataFrames\n    if max_train_samples:\n        train_df = train_df.head(max_train_samples)\n    if max_val_samples:\n        val_df = val_df.head(max_val_samples)\n\n    train_dataset = RISCDataset(image_dir, train_df)\n    val_dataset = RISCDataset(image_dir, val_df)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n    scaler = GradScaler()\n\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        steps = 0\n        optimizer.zero_grad()\n\n        for batch_idx, batch in enumerate(train_loader):\n            try:\n                images = batch[\"images\"]\n                captions = [f\"<image> caption {cap}\" for cap in batch[\"captions\"]]\n\n                inputs = processor(text=captions, images=images, return_tensors=\"pt\", padding=\"longest\").to(device)\n\n                with autocast(\"cuda\"):\n                    outputs = model(\n                        input_ids=inputs[\"input_ids\"],\n                        attention_mask=inputs[\"attention_mask\"],\n                        pixel_values=inputs[\"pixel_values\"],\n                        labels=inputs[\"input_ids\"]\n                    )\n                    loss = outputs.loss / accum_steps\n\n                scaler.scale(loss).backward()\n\n                if (batch_idx + 1) % accum_steps == 0 or (batch_idx + 1) == len(train_loader):\n                    scaler.step(optimizer)\n                    scaler.update()\n                    optimizer.zero_grad()\n\n                total_loss += loss.item() * accum_steps\n                steps += 1\n\n                if steps % 50 == 0:\n                    print(f\"Epoch {epoch+1}, Step {steps}, Loss: {loss.item() * accum_steps:.4f}\")\n\n            except Exception as e:\n                print(f\"Error in batch {batch_idx}: {e}\")\n                continue\n\n        avg_train_loss = total_loss / steps if steps > 0 else 0\n        wandb.log({\"epoch\": epoch+1, \"train_loss\": avg_train_loss})\n\n        # Validation loop\n        model.eval()\n        val_loss = 0\n        val_steps = 0\n        for batch in val_loader:\n            try:\n                images = batch[\"images\"]\n                captions = [f\"<image> caption {cap}\" for cap in batch[\"captions\"]]\n                inputs = processor(text=captions, images=images, return_tensors=\"pt\", padding=\"longest\").to(device)\n\n                with torch.no_grad(), autocast(\"cuda\"):\n                    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n                    val_loss += outputs.loss.item()\n                val_steps += 1\n\n            except Exception as e:\n                print(f\"Validation error: {e}\")\n                continue\n\n        avg_val_loss = val_loss / val_steps if val_steps > 0 else 0\n        wandb.log({\"epoch\": epoch+1, \"val_loss\": avg_val_loss})\n        print(f\"Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}\")\n        model.train()\n\n    model.save_pretrained(output_dir)\n    processor.save_pretrained(output_dir)\n    wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T18:34:22.332793Z","iopub.execute_input":"2025-05-17T18:34:22.333178Z","iopub.status.idle":"2025-05-17T18:34:29.661112Z","shell.execute_reply.started":"2025-05-17T18:34:22.333154Z","shell.execute_reply":"2025-05-17T18:34:29.660514Z"}},"outputs":[{"name":"stderr","text":"2025-05-17 18:34:26.761920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747506866.785510     260 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747506866.792901     260 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Evaluation Functions","metadata":{}},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom rouge_score import rouge_scorer\nfrom transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\nfrom peft import PeftModel \nfrom PIL import Image\nimport torch\nfrom torch.amp import autocast\nimport pandas as pd\nimport os\n\n# Set environment variable for better memory management\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n# Define smoother for BLEU\nsmoothie = SmoothingFunction().method1\n\n\ndef evaluate_zero_shot(model_name, image_dir, val_df, num_samples=10):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    # Load model and processor\n    model = PaliGemmaForConditionalGeneration.from_pretrained(model_name).to(device).to(torch.float16)\n    processor = PaliGemmaProcessor.from_pretrained(model_name)\n\n    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n\n    bleu_scores = []\n    rouge_scores = []\n\n    print(\"Running zero-shot evaluation...\")\n    val_df = val_df.reset_index(drop=True)\n\n    for i in range(min(num_samples, len(val_df))):\n        inputs, output_ids, image = None, None, None\n        try:\n            row = val_df.iloc[i]\n            image_path = os.path.join(image_dir, row.image)\n            if not os.path.exists(image_path):\n                raise FileNotFoundError(f\"Image not found: {image_path}\")\n            \n            image = Image.open(image_path).convert('RGB')\n            prompt = \"<image> caption\"\n\n            inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n\n            with torch.inference_mode(), autocast(\"cuda\", dtype=torch.float16):\n                output_ids = model.generate(\n                    **inputs,\n                    max_new_tokens=50,\n                    num_beams=3,\n                    repetition_penalty=1.2,\n                    eos_token_id=processor.tokenizer.eos_token_id\n                )\n                caption = processor.decode(output_ids[0], skip_special_tokens=True).strip()\n\n            reference = row.caption_1\n            hypothesis = caption\n\n            # Filter out garbage captions\n            if len(hypothesis.split()) < 2 or \"##\" in hypothesis:\n                print(f\"Invalid caption generated at sample {i}. Skipping...\")\n                continue\n\n            # Compute BLEU with smoothing\n            bleu = sentence_bleu([reference.split()], hypothesis.split(), smoothing_function=smoothie)\n            bleu_scores.append(bleu)\n\n            # Compute ROUGE-L\n            rs = scorer.score(reference, hypothesis)['rougeL'].fmeasure\n            rouge_scores.append(rs)\n\n            print(f\"\\nSample {i} - Reference: {reference}\\nHypothesis: {caption}\")\n\n        except Exception as e:\n            print(f\"Error evaluating sample {i}: {e}\")\n        finally:\n            # Clean up memory\n            if 'inputs' in locals():\n                del inputs\n            if 'output_ids' in locals():\n                del output_ids\n            if 'image' in locals():\n                del image\n            torch.cuda.empty_cache()\n\n    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n    avg_rouge = sum(rouge_scores) / len(rouge_scores) if rouge_scores else 0\n\n    print(f\"\\nZero-shot Results: BLEU-4={avg_bleu:.4f}, ROUGE-L={avg_rouge:.4f}\")\n    return {\n        \"zero_shot_BLEU\": avg_bleu,\n        \"zero_shot_ROUGE_L\": avg_rouge\n    }\n\n\ndef evaluate_lora_model(lora_model_path, base_model_name, image_dir, val_df, num_samples=10):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Load base model and apply LoRA weights\n    base_model = PaliGemmaForConditionalGeneration.from_pretrained(base_model_name).to(device).to(torch.float16)\n    model = PeftModel.from_pretrained(base_model, lora_model_path).to(device)\n    processor = PaliGemmaProcessor.from_pretrained(base_model_name)\n\n    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n\n    bleu_scores = []\n    rouge_scores = []\n\n    # Use varied prompts\n    prompt_prefixes = [\n        \"<image> Describe this remote sensing image.\",\n        \"<image> What is shown in this satellite image?\",\n        \"<image> Generate a one-sentence description of this image.\"\n    ]\n\n    print(\"Running LoRA model evaluation...\")\n    val_df = val_df.reset_index(drop=True)\n\n    for i in range(min(num_samples, len(val_df))):\n        inputs, output_ids, image = None, None, None\n        try:\n            row = val_df.iloc[i]\n            image_path = os.path.join(image_dir, row.image)\n            if not os.path.exists(image_path):\n                raise FileNotFoundError(f\"Image not found: {image_path}\")\n            \n            image = Image.open(image_path).convert('RGB')\n            prompt = prompt_prefixes[i % len(prompt_prefixes)]\n\n            inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n\n            with torch.inference_mode(), autocast(\"cuda\", dtype=torch.float16):\n                output_ids = model.generate(\n                    **inputs,\n                    max_new_tokens=50,\n                    num_beams=3,\n                    repetition_penalty=1.2,\n                    no_repeat_ngram_size=3,\n                    eos_token_id=processor.tokenizer.eos_token_id,\n                    pad_token_id=processor.tokenizer.pad_token_id\n                )\n                caption = processor.decode(output_ids[0], skip_special_tokens=True).strip()\n\n            reference = row.caption_1\n            hypothesis = caption\n\n            # Skip if model just repeats the prompt\n            if any(hypothesis.startswith(prefix.replace(\"<image>\", \"\").strip()) for prefix in prompt_prefixes):\n                print(f\"Model repeated the prompt at sample {i}. Skipping...\")\n                continue\n\n            # Skip garbage outputs\n            if len(hypothesis.split()) < 3 or \"##\" in hypothesis or hypothesis.lower().count(\"caption\") > 4:\n                print(f\"Invalid caption generated at sample {i}: '{hypothesis}'. Skipping...\")\n                continue\n\n            # Compute metrics\n            bleu = sentence_bleu([reference.split()], hypothesis.split(), smoothing_function=smoothie)\n            bleu_scores.append(bleu)\n\n            rs = scorer.score(reference, hypothesis)['rougeL'].fmeasure\n            rouge_scores.append(rs)\n\n            print(f\"\\nReference: {reference}\\nHypothesis: {hypothesis}\")\n\n        except Exception as e:\n            print(f\"Error evaluating sample {i}: {e}\")\n        finally:\n            if 'inputs' in locals():\n                del inputs\n            if 'output_ids' in locals():\n                del output_ids\n            if 'image' in locals():\n                del image\n            torch.cuda.empty_cache()\n\n    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n    avg_rouge = sum(rouge_scores) / len(rouge_scores) if rouge_scores else 0\n\n    print(f\"\\nLoRA Model Results: BLEU-4={avg_bleu:.4f}, ROUGE-L={avg_rouge:.4f}\")\n    return {\n        \"lora_BLEU\": avg_bleu,\n        \"lora_ROUGE_L\": avg_rouge\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T18:34:34.927514Z","iopub.execute_input":"2025-05-17T18:34:34.928559Z","iopub.status.idle":"2025-05-17T18:34:35.121419Z","shell.execute_reply.started":"2025-05-17T18:34:34.928527Z","shell.execute_reply":"2025-05-17T18:34:35.120839Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"Train Multiple LoRA Configurations","metadata":{}},{"cell_type":"code","source":"from transformers import PaliGemmaForConditionalGeneration, PaliGemmaProcessor\nfrom peft import PeftModel \n\nimport wandb\nwandb.login()\n\n# Set environment variable for better memory management\nimport os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n# Define evaluation sample size\nnum_samples = 10  # ← Define here to avoid NameError\n\nconfigs = [\n    {\"lora_rank\": 32, \"name\": \"LoRA-R32\"},\n    {\"lora_rank\": 8, \"name\": \"LoRA-R8\"},\n    {\"lora_rank\": 4, \"name\": \"LoRA-R4-k-o\", \"target_modules\": [\"k_proj\", \"o_proj\"]}\n]\n\nresults = []\n\n# Optional: Evaluate Zero-shot Baseline First\nwandb.init(project=\"DI725_Phase2\", name=\"ZeroShot-Baseline\")\nzero_shot_results = evaluate_zero_shot(\n    model_name=\"/kaggle/input/paligemma-2/transformers/paligemma2-3b-pt-224/1\",\n    image_dir=image_dir,\n    val_df=val_df,\n    num_samples=num_samples\n)\nwandb.finish()\n\n# Train and evaluate each LoRA config\nfor config in configs:\n    print(f\"\\n🚀 Training: {config['name']}\")\n    output_dir = f\"./{config['name']}\"\n\n    # Initialize WandB for this experiment\n    wandb.init(\n        project=\"DI725_Phase2\",\n        name=config[\"name\"],\n        config={\n            \"lora_rank\": config.get(\"lora_rank\"),\n            \"target_modules\": \"-\".join(config.get(\"target_modules\", [\"q_proj\", \"v_proj\"])),\n            \"num_samples_eval\": num_samples\n        },\n        reinit=True  # Allows multiple init calls in notebook\n    )\n\n    # Run training\n    train_lora(\n        model_name=\"/kaggle/input/paligemma-2/transformers/paligemma2-3b-pt-224/1\",\n        image_dir=image_dir,\n        caption_file=caption_file,\n        train_df=train_df,\n        val_df=val_df,\n        output_dir=output_dir,\n        lora_rank=config.get(\"lora_rank\", 32),\n        target_modules=config.get(\"target_modules\", [\"q_proj\", \"v_proj\"]),\n        epochs=2,\n        learning_rate=5e-4,\n        batch_size=1,\n        accum_steps=8\n    )\n\n    # Run evaluation\n    result = evaluate_lora_model(\n        output_dir,\n        \"/kaggle/input/paligemma-2/transformers/paligemma2-3b-pt-224/1\",\n        image_dir,\n        val_df,\n        num_samples=num_samples\n    )\n    \n    result[\"config\"] = config[\"name\"]\n    result[\"rank\"] = config.get(\"lora_rank\", 32)\n    result[\"modules\"] = \"-\".join(config.get(\"target_modules\", [\"q_proj\", \"v_proj\"]))\n    results.append(result)\n\n\n    wandb.finish()  # Finish this run before starting a new one","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T18:34:40.900413Z","iopub.execute_input":"2025-05-17T18:34:40.901076Z","iopub.status.idle":"2025-05-17T19:00:08.087847Z","shell.execute_reply.started":"2025-05-17T18:34:40.901048Z","shell.execute_reply":"2025-05-17T19:00:08.086909Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250517_183440-m8566jni</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/m8566jni' target=\"_blank\">ZeroShot-Baseline</a></strong> to <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/m8566jni' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/m8566jni</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d2e4b7b1ab643c2903e29fe88c43e04"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"name":"stdout","text":"Running zero-shot evaluation...\n\nSample 0 - Reference: The storage tanks here are half white and half black .\nHypothesis: caption\nimage result for image of container port in china\n\nSample 1 - Reference: The stratus clouds are located above the surface of the sea .\nHypothesis: caption\ninvention can be seen through the window of invention .\n\nSample 2 - Reference: The long strip island with dense vegetation is surrounded by light blue waters .\nHypothesis: caption\nsatellite image of the island\n\nSample 3 - Reference: The harbor has lots of neatly docked boats and some buildings are next to the harbor .\nHypothesis: caption\nproperty image # directly on the lake with private beach and pontoon\n\nSample 4 - Reference: parking lot next to planted a few trees .\nHypothesis: caption\npolice , ambulance and fire department vehicles line a street as seen in this aerial photo\n\nSample 5 - Reference: A road go across another one diagonally .\nHypothesis: caption\npedestrian bridge over the interstate\n\nSample 6 - Reference: An airport with some staggered runways and buildings in the vacant lot and these runways put a triangular shape .\nHypothesis: caption\naerial view of the runway .\nInvalid caption generated at sample 7. Skipping...\n\nSample 8 - Reference: There is a path between the terraces leading to a building .\nHypothesis: caption\ngeographical feature category in the region\n\nSample 9 - Reference: Two adjacent baseball diamonds next to some parking lots and some buildings beside .\nHypothesis: caption\naerial view of the school\n\nZero-shot Results: BLEU-4=0.0090, ROUGE-L=0.0998\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">ZeroShot-Baseline</strong> at: <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/m8566jni' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/m8566jni</a><br> View project at: <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250517_183440-m8566jni/logs</code>"},"metadata":{}},{"name":"stdout","text":"\n🚀 Training: LoRA-R32\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250517_183515-95nj5eg9</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/95nj5eg9' target=\"_blank\">LoRA-R32</a></strong> to <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/95nj5eg9' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/95nj5eg9</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05bba6237d3f471e8324581735c7485e"}},"metadata":{}},{"name":"stderr","text":"It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Step 50, Loss: 13.5510\nEpoch 1, Step 100, Loss: 12.5343\nEpoch 1, Validation Loss: 12.4339\nEpoch 2, Step 50, Loss: 12.1314\nEpoch 2, Step 100, Loss: 12.0420\nEpoch 2, Validation Loss: 12.0093\nEpoch 3, Step 50, Loss: 12.2082\nEpoch 3, Step 100, Loss: 12.1308\nEpoch 3, Validation Loss: 11.9279\nEpoch 4, Step 50, Loss: 11.6185\nEpoch 4, Step 100, Loss: 11.8060\nEpoch 4, Validation Loss: 11.8727\nEpoch 5, Step 50, Loss: 12.0077\nEpoch 5, Step 100, Loss: 11.8092\nEpoch 5, Validation Loss: 11.8356\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▃▃▅▅▆▆██</td></tr><tr><td>train_loss</td><td>█▂▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_loss</td><td>11.84695</td></tr><tr><td>val_loss</td><td>11.83564</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">LoRA-R32</strong> at: <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/95nj5eg9' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/95nj5eg9</a><br> View project at: <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250517_183515-95nj5eg9/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e60892b21083443abc1e264567e2980a"}},"metadata":{}},{"name":"stdout","text":"Running LoRA model evaluation...\nInvalid caption generated at sample 0: 'Describe this remote sensing image.\ncaption caption caption caption Caption Caption CaptionCaptionCaptionCaption Caption Captioncaption Caption Caption caption caption captions captions captions caption captioncaption caption Caption caption CaptionCaption CaptionCaption caption caption captioned captioned caption captioned caption captionCAPTIONCAPTIONCAPTIONCAPCAPTIONCAPTIONcaptioncaptioncaption captioncaptioncaption'. Skipping...\nInvalid caption generated at sample 1: 'What is shown in this satellite image?\ncaption caption caption caption Caption Caption CaptionCaption CaptionCaptionCaptionCaption Caption Caption caption caption captions captions captions caption caption captioned caption captioned captioned caption captionCAPTIONCAPTIONCAPTIONCAPCAPTIONCAPTIONcaptioncaptioncaption captioncaption caption captions caption captions Caption caption Caption captionCaption caption captioncaption'. Skipping...\n\nReference: The long strip island with dense vegetation is surrounded by light blue waters .\nHypothesis: Generate a one-sentence description of this image.\n☑ ☑ ➤ ➤➤➤ > > > › › ›››› › › > › > > >> >> >> >>> >>> >>> >> >> » » » »> »> »> >> »> >> >> »> »> » » >> » >> >>\nInvalid caption generated at sample 3: 'Describe this remote sensing image.\ncaption caption caption caption Caption Caption CaptionCaptionCaptionCaption Caption Captioncaption Caption Caption caption caption captions captions captions caption caption captioned caption captioned captioned captioned caption captionCAPTIONCAPTIONCAPTIONCAPCAPTIONCAPTIONcaptioncaptioncaption Caption caption Caption captionCaption caption caption cap caption captioncaption caption'. Skipping...\n\nReference: parking lot next to planted a few trees .\nHypothesis: What is shown in this satellite image?\nimage image image image images images images image image imag imag imagimagimagimagImagImagImag ImagImagImagimagImagimagimag imagimagImag imagImagImag imagimag ImagimagImag Imagimag ImagImag Imag Imag Imag imag Imag imag imagImag imag imag\n\nReference: A road go across another one diagonally .\nHypothesis: Generate a one-sentence description of this image.\n☑ ☑ ➤ ➤ > > > >> >> >> >>> >> >>> >>> >>> >> >> » » » »> »> »> » »> » » « « « << << << <<< <<< <<< << <<< << << «< << << « « <<< <<<\nInvalid caption generated at sample 6: 'Describe this remote sensing image.\ncaption caption caption caption Caption Caption CaptionCaptionCaptionCaption Caption Captioncaption Caption Caption caption caption captions captions captions caption caption captioned caption captioned captioned captioned caption captionCAPTIONCAPTIONCAPTIONCAPCAPTIONCAPTIONcaptioncaptioncaption Caption caption Caption captionCaption caption caption cap caption captioncaption caption'. Skipping...\n\nReference: The golf course has some fairways, Roads, Barrier trees and sandpits .\nHypothesis: What is shown in this satellite image?\nimage image image image images images images image image imag imag imagimagimagimagImagImagImag ImagImag Imag Imag Imag imag imag Imag ImagImagImagimagImag Imag imag Imag imagImag imag imagImagImag imagImag Imagimag Imag imagimag imag imag ima\n\nReference: There is a path between the terraces leading to a building .\nHypothesis: Generate a one-sentence description of this image.\n☑ ☑ ➤ ➤ ► ► ►►►►▸▸▸‣‣‣▸▸►▸►►▶▶▶ ▶ ▶ ▶▶▶ ► ► ▸ ▸ ▸ ► ▸ ► ► ▶ ▶ ► ▶ ► ▸ ▶ ▶\nInvalid caption generated at sample 9: 'Describe this remote sensing image.\ncaption caption caption caption Caption Caption CaptionCaptionCaptionCaption Caption Captioncaption Caption Caption caption caption captions captions captions caption caption captioned caption captioned captioned captioned caption captionCAPTIONCAPTIONCAPTIONCAPCAPTIONCAPTIONcaptioncaptioncaption Caption caption Caption captionCaption caption caption cap caption captioncaption caption'. Skipping...\n\nLoRA Model Results: BLEU-4=0.0011, ROUGE-L=0.0744\n\n🚀 Training: LoRA-R8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250517_184333-ih9bd8mt</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/ih9bd8mt' target=\"_blank\">LoRA-R8</a></strong> to <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/ih9bd8mt' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/ih9bd8mt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7fc39dab5534f75ad32db620f61da5c"}},"metadata":{}},{"name":"stdout","text":"Epoch 1, Step 50, Loss: 13.9717\nEpoch 1, Step 100, Loss: 12.4640\nEpoch 1, Validation Loss: 12.4688\nEpoch 2, Step 50, Loss: 11.7667\nEpoch 2, Step 100, Loss: 12.0393\nEpoch 2, Validation Loss: 12.0135\nEpoch 3, Step 50, Loss: 12.0308\nEpoch 3, Step 100, Loss: 11.7334\nEpoch 3, Validation Loss: 11.9261\nEpoch 4, Step 50, Loss: 11.7501\nEpoch 4, Step 100, Loss: 12.0760\nEpoch 4, Validation Loss: 11.8787\nEpoch 5, Step 50, Loss: 11.6648\nEpoch 5, Step 100, Loss: 11.9506\nEpoch 5, Validation Loss: 11.8439\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▃▃▅▅▆▆██</td></tr><tr><td>train_loss</td><td>█▂▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_loss</td><td>11.85417</td></tr><tr><td>val_loss</td><td>11.84387</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">LoRA-R8</strong> at: <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/ih9bd8mt' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/ih9bd8mt</a><br> View project at: <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250517_184333-ih9bd8mt/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fa4a0fc29c34fd689cdf8d78c5ac4be"}},"metadata":{}},{"name":"stdout","text":"Running LoRA model evaluation...\n\nReference: The storage tanks here are half white and half black .\nHypothesis: Describe this remote sensing image.\n☐ ☐ ☐ ☐☐☐☐■■■■■■■■■■■■■■ ■ ■ ■ • • ••••.•.•.•••°•°•°••°•.•°•.•.•°•°•.••°••.•°•••\n\nReference: The stratus clouds are located above the surface of the sea .\nHypothesis: What is shown in this satellite image?\n                     packagepackagepackage package package package packages packages packagespackagespackagespackagespagespagespagespagepagepage page page page\npage page\n\nReference: The long strip island with dense vegetation is surrounded by light blue waters .\nHypothesis: Generate a one-sentence description of this image.\nA A A A a a a aa aa aa aaa aaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\nReference: The harbor has lots of neatly docked boats and some buildings are next to the harbor .\nHypothesis: Describe this remote sensing image.\n                      ▶ ▶ ▶▶▶▶►►►▸▸▸‣‣▸‣▸▸►▸►►‣‣‣►‣►► ► ► ► ▸ ► ▸ ▸ ▸ ► ► ▶ ▶\n\nReference: parking lot next to planted a few trees .\nHypothesis: What is shown in this satellite image?\n              packagepackagepackagepapackagepapapapapapapapapapapapapappappapappappappapapappapapapappapapapapapappapapappapapapa. . . . ...\n\nReference: A road go across another one diagonally .\nHypothesis: Generate a one-sentence description of this image.\n                      ▶ ▶ ▶▶▶▶►►►▸▸▸‣‣▸‣▸▸►▸►►‣►► ► ► ► ▸ ▸ ▸ ► ▸ ► ► ▶ ▶ ► ▶ ► ►\n\nReference: An airport with some staggered runways and buildings in the vacant lot and these runways put a triangular shape .\nHypothesis: Describe this remote sensing image.\n☐ ☐ ☐ ☐☐☐☐■■■■■■■■■■■■■■ ■ ■ ■ • • ••••.•.•.••.••°•.•.•°•.•°•°•°••°•°•°•°•°••°\n\nReference: The golf course has some fairways, Roads, Barrier trees and sandpits .\nHypothesis: What is shown in this satellite image?\n                     ▶ ▶ ▶▶▶▶►►►▸▸▸‣‣▸‣▸▸►▸►► ► ► ► ▸ ▸ ▸ ► ► ▶ ▶ ► ► >> >> >> >>> >>> >>> >> >>\n\nReference: There is a path between the terraces leading to a building .\nHypothesis: Generate a one-sentence description of this image.\n☐ ☐ ☐ ☐☐☐☐■■■■■■■■■■■■■■ ■ ■ ■ • • ••••.•.•.••• • • •• •• •• • ••• • •.•.•°• • •⚫⚫⚫ ⚫\n\nReference: Two adjacent baseball diamonds next to some parking lots and some buildings beside .\nHypothesis: Describe this remote sensing image.\n☐ ☐ ☐ ☐☐☐☐■■■ ■ ■ ■■■■■■■■■■■■■■■■ ■■■ ■ ■■ ■■■■ ■■■▪▪▪ ▪▪▪ ■▪▪■▪ ▪ ▪▪ ▪ ■▪\n\nLoRA Model Results: BLEU-4=0.0024, ROUGE-L=0.0372\n\n🚀 Training: LoRA-R4-k-o\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250517_185153-k2y1p9ik</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/k2y1p9ik' target=\"_blank\">LoRA-R4-k-o</a></strong> to <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/k2y1p9ik' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/k2y1p9ik</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e1da30147c14794bba61c6e168d2a06"}},"metadata":{}},{"name":"stdout","text":"Epoch 1, Step 50, Loss: 14.6810\nEpoch 1, Step 100, Loss: 12.6083\nEpoch 1, Validation Loss: 12.5653\nEpoch 2, Step 50, Loss: 11.8130\nEpoch 2, Step 100, Loss: 11.9576\nEpoch 2, Validation Loss: 12.0209\nEpoch 3, Step 50, Loss: 11.8041\nEpoch 3, Step 100, Loss: 11.8975\nEpoch 3, Validation Loss: 11.9221\nEpoch 4, Step 50, Loss: 11.9578\nEpoch 4, Step 100, Loss: 11.8962\nEpoch 4, Validation Loss: 11.8777\nEpoch 5, Step 50, Loss: 11.5022\nEpoch 5, Step 100, Loss: 12.0072\nEpoch 5, Validation Loss: 11.8561\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▃▃▅▅▆▆██</td></tr><tr><td>train_loss</td><td>█▂▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_loss</td><td>11.85816</td></tr><tr><td>val_loss</td><td>11.85615</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">LoRA-R4-k-o</strong> at: <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/k2y1p9ik' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/k2y1p9ik</a><br> View project at: <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250517_185153-k2y1p9ik/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3062b85898b5468084548a90f5b62672"}},"metadata":{}},{"name":"stdout","text":"Running LoRA model evaluation...\n\nReference: The storage tanks here are half white and half black .\nHypothesis: Describe this remote sensing image.\npackage package package package\n ویکی‌پدیا ویکی‌پدیا ویکی‌پدیا wikipedia wikipedia wikipedia Wikipedia Wikipedia Wikipedia wikipedia wikipediawikipedia wikipedia wikipedia wikiwiki wikiwiki wikipedia wiki wiki wikiwikiwiki wiki wiki wiwiwiwi wiwiwiki wikiwiwi wiki wikiwi wikiwiki wiwi\n\nReference: The stratus clouds are located above the surface of the sea .\nHypothesis: What is shown in this satellite image?\nNone None None None\nNone none none none\nnone none none one one one\none one one ones ones ones\nones ones ones ons ons ons\nons ons ons on on on\nons on on ons ons ona ona ona\nona ona ona\n\nReference: The long strip island with dense vegetation is surrounded by light blue waters .\nHypothesis: Generate a one-sentence description of this image.\npackage package package package packaging packaging packaging package packaging package package packages packages packages packaging packaging packages packagespackagespackagespackages packagespackages packages packaging packagespackages packaging packagingpackaging packaging packagingPackaging packaging packaging Packaging packaging packagingpackages packaging packages packagingpackagespackages packagingpackages packages packagespackaging\n\nReference: The harbor has lots of neatly docked boats and some buildings are next to the harbor .\nHypothesis: Describe this remote sensing image.\npackage package package package\nⓧ pkg pkg pkgpkgpkg pkgpkg pkg pkg\npkg pkg pk pkg pkg pk pk pkg pk\npkgpkgpkgpk pkg pkg pkt pkt pktpkt pkt pkt\npkt pktpktpktpkt pkt pct pct pct\n\nReference: parking lot next to planted a few trees .\nHypothesis: What is shown in this satellite image?\nNone None None None\nNone none none none\nnone none none one one one\nnone one one none one\none one one ones ones ones\none ones ones ons ons ons\nons ons ons on on on\nons on on ons on ons\n\nReference: A road go across another one diagonally .\nHypothesis: Generate a one-sentence description of this image.\npackage package package package\n ویکی‌پدیا ویکی‌پدیا ویکی‌پدیا  ویکی‌پدیا ویکی‌پدیا wikipedia wikipedia wikipediawikipedia wikipedia wikipedia Wikipedia Wikipedia Wikipedia wikipedia Wikipedia wikipedia wikipediaWikipedia Wikipedia Wikipedia Wikimedia Wikimedia WikimediaWikimediaWikimedia WikimediaWikimedia Wikimedia WikimediawikimediawikimediawikimediaWikimediaWikimediaWikimediawikimediaWikimediawikimediawikimedia Wikimedia Wikimedia\n\nReference: An airport with some staggered runways and buildings in the vacant lot and these runways put a triangular shape .\nHypothesis: Describe this remote sensing image.\npackage package package package packaging packaging packaging pkg pkg pkg pk pkg pkgpkgpkg pkgpkg pkg pkgpk pkg pkg pg pg pgpgpgpg pg pg pgs pgs pgspgs pgspgspgs pgs pgs pg pgspgs pg pgspg pgs pgspgpgs\n\nReference: The golf course has some fairways, Roads, Barrier trees and sandpits .\nHypothesis: What is shown in this satellite image?\nNone None None None\nNone none none none\nnone none none one one one\nnone one one none one\none one one ones ones ones\none ones ones ons ons ons\nons ons ons on on on\nons on on ons on ons\n\nReference: There is a path between the terraces leading to a building .\nHypothesis: Generate a one-sentence description of this image.\npackage package package package\n ویکی‌پدیا ویکی‌پدیا ویکی‌پدیا  ویکی‌پدیا ویکی‌پدیا wikipedia wikipedia wikipediawikipedia wikipedia wikipedia Wikipedia Wikipedia Wikipedia wikipedia Wikipedia wikipedia wikipediaWikipedia Wikipedia Wikipedia Wikimedia Wikimedia WikimediaWikimediaWikimedia WikimediaWikimedia Wikimedia WikimediawikimediawikimediawikimediaWikimediaWikimediaWikimediawikimediaWikimediawikimediawikimedia Wikimedia Wikimedia\n\nReference: Two adjacent baseball diamonds next to some parking lots and some buildings beside .\nHypothesis: Describe this remote sensing image.\npackage package package package packaging packaging packaging package packaging package package pkgpkgpkgpkg pkgpkg pkg pkg pkg pk pkg pkg pg pg pgpgpgpg pg pg pgs pg pgs pgs pgspgs pgspgspgspgs pgs pgspgpgs pgspg pgs pgs\n\nLoRA Model Results: BLEU-4=0.0005, ROUGE-L=0.0142\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":" Generate Table","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Add zero-shot baseline to results list\nresults.insert(0, {\n    \"config\": \"Zero-shot Baseline\",\n    \"rank\": \"N/A\",\n    \"modules\": \"N/A\",\n    \"lora_BLEU\": zero_shot_results[\"zero_shot_BLEU\"],\n    \"lora_ROUGE_L\": zero_shot_results[\"zero_shot_ROUGE_L\"]\n})\n\n# Create DataFrame\nresults_table = pd.DataFrame(results)[[\"config\", \"rank\", \"modules\", \"lora_BLEU\", \"lora_ROUGE_L\"]]\nresults_table.columns = [\"Model\", \"Rank\", \"Modules\", \"BLEU-4\", \"ROUGE-L\"]\n\nprint(\"\\n📊 Model Performance Comparison:\")\nprint(results_table.to_markdown(index=False))\n\n# Save to CSV\nresults_table.to_csv(\"results_table.csv\", index=False)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}