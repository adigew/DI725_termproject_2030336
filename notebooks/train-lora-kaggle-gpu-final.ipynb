{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11699725,"sourceType":"datasetVersion","datasetId":7343599},{"sourceId":185693,"sourceType":"modelInstanceVersion","modelInstanceId":158308,"modelId":164716}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T13:35:26.437351Z","iopub.execute_input":"2025-05-16T13:35:26.437666Z","iopub.status.idle":"2025-05-16T13:35:26.444986Z","shell.execute_reply.started":"2025-05-16T13:35:26.437614Z","shell.execute_reply":"2025-05-16T13:35:26.444209Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.model_download(\"google/paligemma-2/transformers/paligemma2-3b-pt-224\")\n\nprint(\"Path to model files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T13:35:29.476205Z","iopub.execute_input":"2025-05-16T13:35:29.476915Z","iopub.status.idle":"2025-05-16T13:35:30.383428Z","shell.execute_reply.started":"2025-05-16T13:35:29.476888Z","shell.execute_reply":"2025-05-16T13:35:30.382567Z"}},"outputs":[{"name":"stdout","text":"Path to model files: /kaggle/input/paligemma-2/transformers/paligemma2-3b-pt-224/1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Install dependencies\n!pip install -q transformers peft nltk rouge-score wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T13:35:32.494756Z","iopub.execute_input":"2025-05-16T13:35:32.495379Z","iopub.status.idle":"2025-05-16T13:36:49.872225Z","shell.execute_reply.started":"2025-05-16T13:35:32.495354Z","shell.execute_reply":"2025-05-16T13:36:49.871239Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Verify GPU\nimport torch\nprint(torch.__version__)\nprint(torch.cuda.is_available())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T13:38:19.109942Z","iopub.execute_input":"2025-05-16T13:38:19.110329Z","iopub.status.idle":"2025-05-16T13:38:22.314349Z","shell.execute_reply.started":"2025-05-16T13:38:19.110298Z","shell.execute_reply":"2025-05-16T13:38:22.313532Z"}},"outputs":[{"name":"stdout","text":"2.5.1+cu124\nTrue\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"d070aabfe54f4733fb727662604b037dee34842c\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T13:40:45.969541Z","iopub.execute_input":"2025-05-16T13:40:45.970255Z","iopub.status.idle":"2025-05-16T13:40:46.113879Z","shell.execute_reply.started":"2025-05-16T13:40:45.970220Z","shell.execute_reply":"2025-05-16T13:40:46.113090Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n\ndata = pd.read_csv(\"/kaggle/input/riscmm/RISCM/captions.csv\")\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T12:11:48.727777Z","iopub.execute_input":"2025-05-16T12:11:48.728241Z","iopub.status.idle":"2025-05-16T12:11:49.252801Z","shell.execute_reply.started":"2025-05-16T12:11:48.728217Z","shell.execute_reply":"2025-05-16T12:11:49.252094Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"  source split           image  \\\n0   NWPU  test  NWPU_31430.jpg   \n1   NWPU  test  NWPU_31431.jpg   \n2   NWPU  test  NWPU_31432.jpg   \n3   NWPU  test  NWPU_31433.jpg   \n4   NWPU  test  NWPU_31434.jpg   \n\n                                           caption_1  \\\n0   A gray plane on the runway and the lawn beside .   \n1  Three small planes parked in a line on the air...   \n2  A plane parked in a line on the airport with s...   \n3  A small plane and a big plane parked next to b...   \n4       Two planes parked next to boarding bridges .   \n\n                                           caption_2  \\\n0        A grey plane is on the runway by the lawn .   \n1  There are four aircraft on the open ground, Th...   \n2  A white plane was parked on the instruction li...   \n3  A white plane and a gray plane parked at the b...   \n4  Two aircraft were parked at the departure gates .   \n\n                                           caption_3  \\\n0  There is an airplane on the runway with a larg...   \n1  There are many planes of different sizes in a ...   \n2  An airplane parked in an open area with many c...   \n3  Two planes of different sizes are neatly parke...   \n4  Two planes of different sizes are neatly parke...   \n\n                                           caption_4  \\\n0  A plane is parked on the runway next to the gr...   \n1             Four planes are parked on the runway .   \n2              A plane is parked on the open space .   \n3  A large plane and a small plane are parked nea...   \n4       Two planes are parked next to the terminal .   \n\n                                           caption_5  \n0  There is a plane on the runway beside the grass .  \n1  Four planes of different sizes were on the mar...  \n2            There is 1 plane on the ground marked .  \n3              Two planes are on the marked ground .  \n4              Two planes are on the marked ground .  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>split</th>\n      <th>image</th>\n      <th>caption_1</th>\n      <th>caption_2</th>\n      <th>caption_3</th>\n      <th>caption_4</th>\n      <th>caption_5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NWPU</td>\n      <td>test</td>\n      <td>NWPU_31430.jpg</td>\n      <td>A gray plane on the runway and the lawn beside .</td>\n      <td>A grey plane is on the runway by the lawn .</td>\n      <td>There is an airplane on the runway with a larg...</td>\n      <td>A plane is parked on the runway next to the gr...</td>\n      <td>There is a plane on the runway beside the grass .</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NWPU</td>\n      <td>test</td>\n      <td>NWPU_31431.jpg</td>\n      <td>Three small planes parked in a line on the air...</td>\n      <td>There are four aircraft on the open ground, Th...</td>\n      <td>There are many planes of different sizes in a ...</td>\n      <td>Four planes are parked on the runway .</td>\n      <td>Four planes of different sizes were on the mar...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NWPU</td>\n      <td>test</td>\n      <td>NWPU_31432.jpg</td>\n      <td>A plane parked in a line on the airport with s...</td>\n      <td>A white plane was parked on the instruction li...</td>\n      <td>An airplane parked in an open area with many c...</td>\n      <td>A plane is parked on the open space .</td>\n      <td>There is 1 plane on the ground marked .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NWPU</td>\n      <td>test</td>\n      <td>NWPU_31433.jpg</td>\n      <td>A small plane and a big plane parked next to b...</td>\n      <td>A white plane and a gray plane parked at the b...</td>\n      <td>Two planes of different sizes are neatly parke...</td>\n      <td>A large plane and a small plane are parked nea...</td>\n      <td>Two planes are on the marked ground .</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NWPU</td>\n      <td>test</td>\n      <td>NWPU_31434.jpg</td>\n      <td>Two planes parked next to boarding bridges .</td>\n      <td>Two aircraft were parked at the departure gates .</td>\n      <td>Two planes of different sizes are neatly parke...</td>\n      <td>Two planes are parked next to the terminal .</td>\n      <td>Two planes are on the marked ground .</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"Load Dataset with Small Partition","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\n\ndef load_small_partition(image_dir, caption_file, sample_size=200):\n    df = pd.read_csv(caption_file)\n\n    # Filter out missing images\n    valid_images = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n    df = df[df['image'].isin(valid_images)]\n\n    # Use existing splits\n    train_df = df[df['split'] == 'train'].sample(frac=1, random_state=42).head(sample_size)\n    val_df = df[df['split'] == 'test'].sample(frac=1, random_state=42).head(int(0.2 * sample_size))\n\n    print(f\"Loaded small partition: {len(train_df)} train, {len(val_df)} val\")\n    return train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n\n# Set paths\nimage_dir = \"/kaggle/input/riscmm/RISCM/resized\"\ncaption_file = \"/kaggle/input/riscmm/RISCM/captions.csv\"\n\n# Load data\ntrain_df, val_df = load_small_partition(image_dir, caption_file, sample_size=200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T13:38:36.880215Z","iopub.execute_input":"2025-05-16T13:38:36.880622Z","iopub.status.idle":"2025-05-16T13:39:35.580977Z","shell.execute_reply.started":"2025-05-16T13:38:36.880600Z","shell.execute_reply":"2025-05-16T13:39:35.580079Z"}},"outputs":[{"name":"stdout","text":"Loaded small partition: 200 train, 40 val\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Training Function","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import PaliGemmaForConditionalGeneration, PaliGemmaProcessor\nfrom peft import LoraConfig, get_peft_model\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.amp import autocast, GradScaler\nimport torch.optim as optim\n\nclass RISCDataset(Dataset):\n    def __init__(self, image_dir, df):\n        self.image_dir = image_dir\n        self.df = df\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = os.path.join(self.image_dir, row.image)\n        image = Image.open(image_path).convert('RGB')\n        caption = row.caption_1\n        return {\"image\": image, \"caption\": caption}\n\ndef custom_collate_fn(batch):\n    images = [item[\"image\"] for item in batch]\n    captions = [item[\"caption\"] for item in batch]\n    return {\"images\": images, \"captions\": captions}\n\ndef train_lora(model_name, image_dir, train_df, val_df, caption_file, output_dir,\n               lora_rank=4, epochs=3, learning_rate=1e-4,\n               max_train_samples=None, max_val_samples=None,\n               batch_size=1, accum_steps=8,\n               target_modules=[\"q_proj\", \"v_proj\"]):\n\n    wandb.init(project=\"DI725_Phase2\", name=f\"LoRA-R{lora_rank}\")\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = PaliGemmaForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n    processor = PaliGemmaProcessor.from_pretrained(model_name, use_fast=True)\n\n    lora_config = LoraConfig(\n        r=lora_rank,\n        lora_alpha=32,\n        target_modules=target_modules,\n        lora_dropout=0.1\n    )\n    model = get_peft_model(model, lora_config)\n\n    # Use provided train/val DataFrames\n    if max_train_samples:\n        train_df = train_df.head(max_train_samples)\n    if max_val_samples:\n        val_df = val_df.head(max_val_samples)\n\n    train_dataset = RISCDataset(image_dir, train_df)\n    val_dataset = RISCDataset(image_dir, val_df)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n    scaler = GradScaler()\n\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        steps = 0\n        optimizer.zero_grad()\n\n        for batch_idx, batch in enumerate(train_loader):\n            try:\n                images = batch[\"images\"]\n                captions = [f\"<image> caption {cap}\" for cap in batch[\"captions\"]]\n\n                inputs = processor(text=captions, images=images, return_tensors=\"pt\", padding=\"longest\").to(device)\n\n                with autocast(\"cuda\"):\n                    outputs = model(\n                        input_ids=inputs[\"input_ids\"],\n                        attention_mask=inputs[\"attention_mask\"],\n                        pixel_values=inputs[\"pixel_values\"],\n                        labels=inputs[\"input_ids\"]\n                    )\n                    loss = outputs.loss / accum_steps\n\n                scaler.scale(loss).backward()\n\n                if (batch_idx + 1) % accum_steps == 0 or (batch_idx + 1) == len(train_loader):\n                    scaler.step(optimizer)\n                    scaler.update()\n                    optimizer.zero_grad()\n\n                total_loss += loss.item() * accum_steps\n                steps += 1\n\n                if steps % 50 == 0:\n                    print(f\"Epoch {epoch+1}, Step {steps}, Loss: {loss.item() * accum_steps:.4f}\")\n\n            except Exception as e:\n                print(f\"Error in batch {batch_idx}: {e}\")\n                continue\n\n        avg_train_loss = total_loss / steps if steps > 0 else 0\n        wandb.log({\"epoch\": epoch+1, \"train_loss\": avg_train_loss})\n\n        # Validation loop\n        model.eval()\n        val_loss = 0\n        val_steps = 0\n        for batch in val_loader:\n            try:\n                images = batch[\"images\"]\n                captions = [f\"<image> caption {cap}\" for cap in batch[\"captions\"]]\n                inputs = processor(text=captions, images=images, return_tensors=\"pt\", padding=\"longest\").to(device)\n\n                with torch.no_grad(), autocast(\"cuda\"):\n                    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n                    val_loss += outputs.loss.item()\n                val_steps += 1\n\n            except Exception as e:\n                print(f\"Validation error: {e}\")\n                continue\n\n        avg_val_loss = val_loss / val_steps if val_steps > 0 else 0\n        wandb.log({\"epoch\": epoch+1, \"val_loss\": avg_val_loss})\n        print(f\"Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}\")\n        model.train()\n\n    model.save_pretrained(output_dir)\n    processor.save_pretrained(output_dir)\n    wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T13:40:12.625115Z","iopub.execute_input":"2025-05-16T13:40:12.625468Z","iopub.status.idle":"2025-05-16T13:40:34.657851Z","shell.execute_reply.started":"2025-05-16T13:40:12.625444Z","shell.execute_reply":"2025-05-16T13:40:34.657188Z"}},"outputs":[{"name":"stderr","text":"2025-05-16 13:40:22.387653: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747402822.591277      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747402822.647998      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"Evaluation Functions","metadata":{}},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom rouge_score import rouge_scorer\nfrom transformers import PaliGemmaForConditionalGeneration, PaliGemmaProcessor\nfrom peft import PeftModel \nfrom PIL import Image\nimport torch\nfrom torch.amp import autocast\nimport pandas as pd\nimport os\n\n# Define smoother for BLEU\nsmoothie = SmoothingFunction().method1\n\ndef evaluate_zero_shot(model_name, image_dir, val_df, num_samples=50):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model = PaliGemmaForConditionalGeneration.from_pretrained(model_name).to(device)\n    processor = PaliGemmaProcessor.from_pretrained(model_name, use_fast=True)\n    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n\n    bleu_scores = []\n    rouge_scores = []\n\n    print(\"Running zero-shot evaluation...\")\n    val_df = val_df.reset_index(drop=True)\n\n    for i in range(min(num_samples, len(val_df))):\n        try:\n            row = val_df.iloc[i]\n            image_path = os.path.join(image_dir, row.image)\n            if not os.path.exists(image_path):\n                raise FileNotFoundError(f\"Image not found: {image_path}\")\n            image = Image.open(image_path).convert('RGB')\n            prompt = \"<image> caption\"\n\n            inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n\n            with torch.no_grad(), autocast(\"cuda\"):\n                output_ids = model.generate(**inputs, max_new_tokens=50)\n                caption = processor.decode(output_ids[0], skip_special_tokens=True)\n\n            reference = row.caption_1\n            hypothesis = caption\n\n            # BLEU with smoothing\n            bleu = sentence_bleu([reference.split()], hypothesis.split(), smoothing_function=smoothie)\n            bleu_scores.append(bleu)\n\n            # ROUGE-L\n            rs = scorer.score(reference, hypothesis)['rougeL'].fmeasure\n            rouge_scores.append(rs)\n\n            print(f\"\\nReference: {reference}\\nHypothesis: {hypothesis}\")\n\n        except Exception as e:\n            print(f\"Error evaluating sample {i}: {e}\")\n            continue\n\n    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n    avg_rouge = sum(rouge_scores) / len(rouge_scores) if rouge_scores else 0\n\n    print(f\"\\nZero-shot Results: BLEU-4={avg_bleu:.4f}, ROUGE-L={avg_rouge:.4f}\")\n\n    return {\n        \"zero_shot_BLEU\": avg_bleu,\n        \"zero_shot_ROUGE_L\": avg_rouge\n    }\n\n\ndef evaluate_lora_model(lora_model_path, base_model_name, image_dir, val_df, num_samples=50):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    base_model = PaliGemmaForConditionalGeneration.from_pretrained(base_model_name).to(device)\n    model = PeftModel.from_pretrained(base_model, lora_model_path).to(device)\n    processor = PaliGemmaProcessor.from_pretrained(base_model_name)\n    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n\n    bleu_scores = []\n    rouge_scores = []\n\n    print(\"Running LoRA model evaluation...\")\n    val_df = val_df.reset_index(drop=True)\n\n    for i in range(min(num_samples, len(val_df))):\n        try:\n            row = val_df.iloc[i]\n            image_path = os.path.join(image_dir, row.image)\n            if not os.path.exists(image_path):\n                raise FileNotFoundError(f\"Image not found: {image_path}\")\n            image = Image.open(image_path).convert('RGB')\n            prompt = \"<image> caption\"\n\n            inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n\n            with torch.no_grad(), autocast(\"cuda\"):\n                output_ids = model.generate(**inputs, max_new_tokens=50)\n                caption = processor.decode(output_ids[0], skip_special_tokens=True)\n\n            reference = row.caption_1\n            hypothesis = caption\n\n            # BLEU with smoothing\n            bleu = sentence_bleu([reference.split()], hypothesis.split(), smoothing_function=smoothie)\n            bleu_scores.append(bleu)\n\n            # ROUGE-L\n            rs = scorer.score(reference, hypothesis)['rougeL'].fmeasure\n            rouge_scores.append(rs)\n\n            print(f\"\\nReference: {reference}\\nHypothesis: {hypothesis}\")\n\n        except Exception as e:\n            print(f\"Error evaluating sample {i}: {e}\")\n            continue\n\n    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n    avg_rouge = sum(rouge_scores) / len(rouge_scores) if rouge_scores else 0\n\n    print(f\"\\nLoRA Model Results: BLEU-4={avg_bleu:.4f}, ROUGE-L={avg_rouge:.4f}\")\n\n    return {\n        \"lora_BLEU\": avg_bleu,\n        \"lora_ROUGE_L\": avg_rouge\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T13:40:52.830272Z","iopub.execute_input":"2025-05-16T13:40:52.830874Z","iopub.status.idle":"2025-05-16T13:40:53.495352Z","shell.execute_reply.started":"2025-05-16T13:40:52.830847Z","shell.execute_reply":"2025-05-16T13:40:53.494691Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"Inference Visualization","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef visualize_inference(model, processor, val_df, image_dir, num_samples=5):\n    fig, axes = plt.subplots(num_samples, 1, figsize=(10, 5 * num_samples))\n    val_df = val_df.reset_index(drop=True)\n\n    for i in range(min(num_samples, len(val_df))):\n        row = val_df.iloc[i]\n        image_path = os.path.join(image_dir, row.image)\n        image = Image.open(image_path).convert('RGB')\n        prompt = \"<image> caption\"\n\n        inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(\"cuda\")\n        with torch.no_grad(), autocast(\"cuda\"):\n            output_ids = model.generate(**inputs, max_new_tokens=50)\n            caption = processor.decode(output_ids[0], skip_special_tokens=True)\n\n        ax = axes[i] if num_samples > 1 else axes\n        ax.imshow(image)\n        ax.axis(\"off\")\n        ax.set_title(f\"True: {row.caption_1}\\nPred: {caption}\", fontsize=10)\n\n    plt.tight_layout()\n    plt.savefig(\"generated_captions.png\", dpi=300)\n    plt.show()\n\n    # Log to WandB\n    wandb.log({\"Generated Captions\": [wandb.Image(\"generated_captions.png\")]})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T13:41:13.557430Z","iopub.execute_input":"2025-05-16T13:41:13.557771Z","iopub.status.idle":"2025-05-16T13:41:13.566315Z","shell.execute_reply.started":"2025-05-16T13:41:13.557748Z","shell.execute_reply":"2025-05-16T13:41:13.565494Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"Train Multiple LoRA Configurations","metadata":{}},{"cell_type":"code","source":"import wandb\nwandb.login()\n\n# Set environment variable for better memory management\nimport os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n# Define evaluation sample size\nnum_samples = 10  # ‚Üê Define here to avoid NameError\n\nconfigs = [\n    {\"lora_rank\": 4, \"name\": \"LoRA-R4\"},\n    {\"lora_rank\": 8, \"name\": \"LoRA-R8\"},\n    {\"lora_rank\": 4, \"name\": \"LoRA-R4-k-o\", \"target_modules\": [\"k_proj\", \"o_proj\"]}\n]\n\nresults = []\n\n# Optional: Evaluate Zero-shot Baseline First\nwandb.init(project=\"DI725_Phase2\", name=\"ZeroShot-Baseline\")\nzero_shot_results = evaluate_zero_shot(\n    model_name=\"/kaggle/input/paligemma-2/transformers/paligemma2-3b-pt-224/1\",\n    image_dir=image_dir,\n    val_df=val_df,\n    num_samples=num_samples  # ‚Üê Now defined\n)\nwandb.log({\n    \"zero_shot_BLEU\": zero_shot_results[\"zero_shot_BLEU\"],\n    \"zero_shot_ROUGE_L\": zero_shot_results[\"zero_shot_ROUGE_L\"]\n})\nwandb.finish()\n\n# Train and evaluate each LoRA config\nfor config in configs:\n    print(f\"\\nüöÄ Training: {config['name']}\")\n    output_dir = f\"./{config['name']}\"\n\n    wandb.init(\n        project=\"DI725_Phase2\",\n        name=config[\"name\"],\n        config={\n            \"lora_rank\": config.get(\"lora_rank\"),\n            \"target_modules\": \"-\".join(config.get(\"target_modules\", [\"q_proj\", \"v_proj\"])),\n            \"num_samples_eval\": num_samples\n        },\n        reinit=True\n    )\n\n    # Run training\n    train_lora(\n        model_name=\"/kaggle/input/paligemma-2/transformers/paligemma2-3b-pt-224/1\",\n        image_dir=image_dir,\n        caption_file=caption_file,\n        train_df=train_df,\n        val_df=val_df,\n        output_dir=output_dir,\n        lora_rank=config.get(\"lora_rank\", 4),\n        target_modules=config.get(\"target_modules\", [\"q_proj\", \"v_proj\"]),\n        epochs=1,\n        learning_rate=1e-4,\n        batch_size=1,\n        accum_steps=8\n    )\n\n    # Run evaluation\n    result = evaluate_lora_model(\n        output_dir,\n        \"/kaggle/input/paligemma-2/transformers/paligemma2-3b-pt-224/1\",\n        image_dir,\n        val_df,\n        num_samples=num_samples  # Pass consistent value\n    )\n    result[\"config\"] = config[\"name\"]\n    result[\"rank\"] = config.get(\"lora_rank\", 4)\n    result[\"modules\"] = \"-\".join(config.get(\"target_modules\", [\"q_proj\", \"v_proj\"]))\n    results.append(result)\n\n    # Log metrics\n    wandb.log({\n        \"lora_BLEU\": result[\"lora_BLEU\"],\n        \"lora_ROUGE_L\": result[\"lora_ROUGE_L\"]\n    })\n    wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T13:41:20.729808Z","iopub.execute_input":"2025-05-16T13:41:20.730129Z","iopub.status.idle":"2025-05-16T13:52:24.314372Z","shell.execute_reply.started":"2025-05-16T13:41:20.730106Z","shell.execute_reply":"2025-05-16T13:52:24.313192Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250516_134120-5vx1old7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/5vx1old7' target=\"_blank\">ZeroShot-Baseline</a></strong> to <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/5vx1old7' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/5vx1old7</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e4e1f75defb40528d78cdcad1842246"}},"metadata":{}},{"name":"stdout","text":"Running zero-shot evaluation...\nError evaluating sample 0: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 45.12 MiB is free. Process 4454 has 15.84 GiB memory in use. Of the allocated memory 15.51 GiB is allocated by PyTorch, and 46.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError evaluating sample 1: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 23.12 MiB is free. Process 4454 has 15.86 GiB memory in use. Of the allocated memory 15.51 GiB is allocated by PyTorch, and 66.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError evaluating sample 2: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 43.12 MiB is free. Process 4454 has 15.84 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 109.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError evaluating sample 3: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 23.12 MiB is free. Process 4454 has 15.86 GiB memory in use. Of the allocated memory 15.51 GiB is allocated by PyTorch, and 66.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError evaluating sample 4: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 43.12 MiB is free. Process 4454 has 15.84 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 109.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError evaluating sample 5: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 23.12 MiB is free. Process 4454 has 15.86 GiB memory in use. Of the allocated memory 15.51 GiB is allocated by PyTorch, and 66.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError evaluating sample 6: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 43.12 MiB is free. Process 4454 has 15.84 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 109.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError evaluating sample 7: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 23.12 MiB is free. Process 4454 has 15.86 GiB memory in use. Of the allocated memory 15.51 GiB is allocated by PyTorch, and 66.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError evaluating sample 8: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 43.12 MiB is free. Process 4454 has 15.84 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 109.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError evaluating sample 9: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 23.12 MiB is free. Process 4454 has 15.86 GiB memory in use. Of the allocated memory 15.51 GiB is allocated by PyTorch, and 66.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\nZero-shot Results: BLEU-4=0.0000, ROUGE-L=0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>zero_shot_BLEU</td><td>‚ñÅ</td></tr><tr><td>zero_shot_ROUGE_L</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>zero_shot_BLEU</td><td>0</td></tr><tr><td>zero_shot_ROUGE_L</td><td>0</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">ZeroShot-Baseline</strong> at: <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/5vx1old7' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/5vx1old7</a><br> View project at: <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250516_134120-5vx1old7/logs</code>"},"metadata":{}},{"name":"stdout","text":"\nüöÄ Training: LoRA-R4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250516_134227-s2a6z0vj</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/s2a6z0vj' target=\"_blank\">LoRA-R4</a></strong> to <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/s2a6z0vj' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/s2a6z0vj</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c7140d46617407499bd826b1a0f98ca"}},"metadata":{}},{"name":"stderr","text":"It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Step 50, Loss: 16.5566\nEpoch 1, Step 100, Loss: 14.6721\nEpoch 1, Step 150, Loss: 13.6143\nEpoch 1, Step 200, Loss: 12.6525\nEpoch 1, Validation Loss: 12.8440\nEpoch 2, Step 50, Loss: 12.6183\nEpoch 2, Step 100, Loss: 12.2363\nEpoch 2, Step 150, Loss: 11.9825\nEpoch 2, Step 200, Loss: 11.8484\nEpoch 2, Validation Loss: 12.1768\nEpoch 3, Step 50, Loss: 12.1753\nEpoch 3, Step 100, Loss: 11.6658\nEpoch 3, Step 150, Loss: 12.2592\nEpoch 3, Step 200, Loss: 12.2358\nEpoch 3, Validation Loss: 11.9697\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñà‚ñà</td></tr><tr><td>train_loss</td><td>‚ñà‚ñÇ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÉ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>train_loss</td><td>12.06554</td></tr><tr><td>val_loss</td><td>11.96967</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">LoRA-R4</strong> at: <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/s2a6z0vj' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2/runs/s2a6z0vj</a><br> View project at: <a href='https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/DI725_Phase2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250516_134227-s2a6z0vj/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b37969c2e62847559a5c6c19b9f8cd08"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"name":"stdout","text":"Running LoRA model evaluation...\n\nReference: The storage tanks here are half white and half black .\nHypothesis:  caption\nA A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n\nReference: The stratus clouds are located above the surface of the sea .\nHypothesis:  caption\nA A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n\nReference: The long strip island with dense vegetation is surrounded by light blue waters .\nHypothesis:  caption\nA A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n\nReference: The harbor has lots of neatly docked boats and some buildings are next to the harbor .\nHypothesis:  caption\n‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß\n\nReference: parking lot next to planted a few trees .\nHypothesis:  caption\n‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß‚ìß\n\nReference: A road go across another one diagonally .\nHypothesis:  caption\nA A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n\nReference: An airport with some staggered runways and buildings in the vacant lot and these runways put a triangular shape .\nHypothesis:  caption\nA A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n\nReference: The golf course has some fairways, Roads, Barrier trees and sandpits .\nHypothesis:  caption\nA A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n\nReference: There is a path between the terraces leading to a building .\nHypothesis:  caption\nA A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n\nReference: Two adjacent baseball diamonds next to some parking lots and some buildings beside .\nHypothesis:  caption\nA A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n\nLoRA Model Results: BLEU-4=0.0004, ROUGE-L=0.0128\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3747302394.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# Log metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     wandb.log({\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;34m\"lora_BLEU\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lora_BLEU\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;34m\"lora_ROUGE_L\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lora_ROUGE_L\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/preinit.py\u001b[0m in \u001b[0;36mpreinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m ) -> Callable:\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You must call wandb.init() before {name}()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"],"ename":"Error","evalue":"You must call wandb.init() before wandb.log()","output_type":"error"}],"execution_count":11},{"cell_type":"markdown","source":" Generate Table","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Format results\nresults_table = pd.DataFrame(results)\nresults_table = results_table.rename(columns={\n    \"lora_BLEU\": \"BLEU-4\",\n    \"lora_ROUGE_L\": \"ROUGE-L\"\n})[[\"config\", \"rank\", \"modules\", \"BLEU-4\", \"ROUGE-L\"]]\n\n# Add zero-shot baseline\nzero_shot_row = {\n    \"config\": \"Zero-shot Baseline\",\n    \"rank\": \"N/A\",\n    \"modules\": \"N/A\",\n    \"BLEU-4\": f\"{zero_shot_results['zero_shot_BLEU']:.4f}\",\n    \"ROUGE-L\": f\"{zero_shot_results['zero_shot_ROUGE_L']:.4f}\"\n}\n\n# Prepend zero-shot\nresults_table = pd.concat([pd.DataFrame([zero_shot_row]), results_table], ignore_index=True)\n\nprint(\"\\nüìä Model Performance Comparison:\")\nprint(results_table.to_markdown(index=False))\n\n# Save to CSV\nresults_table.to_csv(\"results_table.csv\", index=False)\n\n# Upload to WandB\nartifact = wandb.Artifact(\"results_table\", type=\"evaluation\")\nartifact.add(wandb.Table(dataframe=results_table), \"results_table\")\nwandb.init(project=\"DI725_Phase2\", name=\"FinalResults\")\nwandb.log_artifact(artifact)\nwandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Visualize Sample Captions from Best Model","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplot_df = results_table[results_table[\"config\"] != \"Zero-shot Baseline\"].copy()\nplot_df[\"BLEU-4\"] = plot_df[\"BLEU-4\"].astype(float)\nplot_df[\"ROUGE-L\"] = plot_df[\"ROUGE-L\"].astype(float)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(data=plot_df, x=\"config\", y=\"BLEU-4\", label=\"BLEU-4\", ci=None)\nsns.barplot(data=plot_df, x=\"config\", y=\"ROUGE-L\", label=\"ROUGE-L\", ci=None, alpha=0.6)\n\nplt.xticks(rotation=45)\nplt.title(\"Model Performance Comparison\")\nplt.ylabel(\"Score\")\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_model_path = \"./LoRA-R8\"\nbase_model_name = \"/kaggle/input/paligemma-2/transformers/paligemma2-3b-pt-224/1\"\nprocessor = PaliGemmaProcessor.from_pretrained(base_model_name)\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(best_model_path).to(\"cuda\")\n\nvisualize_inference(model, processor, val_df, image_dir, num_samples=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}